{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8a_train_sqil_sac.ipynb)\n",
    "# Train an Agent using Soft Q Imitation Learning with SAC\n",
    "\n",
    "In the previous tutorial, we used Soft Q Imitation Learning ([SQIL](https://arxiv.org/abs/1905.11108)) on top of the DQN base algorithm. In fact, SQIL can be combined with any off-policy algorithm from `stable_baselines3`. Here, we train a Pendulum agent using SQIL + SAC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need some expert trajectories in our environment (`Pendulum-v1`).\n",
    "Note that you can use other environments, but the action space must be continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from imitation.data import huggingface_utils\n",
    "\n",
    "# Download some expert trajectories from the HuggingFace Datasets Hub.\n",
    "dataset = datasets.load_dataset(\"HumanCompatibleAI/ppo-Pendulum-v1\")\n",
    "\n",
    "# Convert the dataset to a format usable by the imitation library.\n",
    "expert_trajectories = huggingface_utils.TrajectoryDatasetSequence(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check if the expert is any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 06:11:13.680320: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/lorangpi/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 200 trajectories. The average length of each trajectory is 200.0. The average return of each trajectory is -205.22814517737746.\n"
     ]
    }
   ],
   "source": [
    "from imitation.data import rollout\n",
    "\n",
    "trajectory_stats = rollout.rollout_stats(expert_trajectories)\n",
    "\n",
    "print(\n",
    "    f\"We have {trajectory_stats['n_traj']} trajectories. \"\n",
    "    f\"The average length of each trajectory is {trajectory_stats['len_mean']}. \"\n",
    "    f\"The average return of each trajectory is {trajectory_stats['return_mean']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we collected our expert trajectories, it's time to set up our imitation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Soft Q Imitation Learning (SQIL) (https://arxiv.org/abs/1905.11108).\n",
    "\n",
    "Trains a policy via DQN-style Q-learning,\n",
    "replacing half the buffer with expert demonstrations and adjusting the rewards.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import dqn\n",
    "from stable_baselines3.common import (\n",
    "    buffers,\n",
    "    off_policy_algorithm,\n",
    "    policies,\n",
    "    type_aliases,\n",
    "    vec_env,\n",
    ")\n",
    "\n",
    "from imitation.algorithms import base as algo_base\n",
    "from imitation.data import rollout, types\n",
    "from imitation.util import logger, util\n",
    "\n",
    "\n",
    "class SQIL(algo_base.DemonstrationAlgorithm[types.Transitions]):\n",
    "    \"\"\"Soft Q Imitation Learning (SQIL).\n",
    "\n",
    "    Trains a policy via DQN-style Q-learning,\n",
    "    replacing half the buffer with expert demonstrations and adjusting the rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    expert_buffer: buffers.ReplayBuffer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        venv: vec_env.VecEnv,\n",
    "        demonstrations: Optional[algo_base.AnyTransitions],\n",
    "        policy: Union[str, Type[policies.BasePolicy]],\n",
    "        custom_logger: Optional[logger.HierarchicalLogger] = None,\n",
    "        rl_algo_class: Type[off_policy_algorithm.OffPolicyAlgorithm] = dqn.DQN,\n",
    "        rl_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        \"\"\"Builds SQIL.\n",
    "\n",
    "        Args:\n",
    "            venv: The vectorized environment to train on.\n",
    "            demonstrations: Demonstrations to use for training.\n",
    "            policy: The policy model to use (SB3).\n",
    "            custom_logger: Where to log to; if None (default), creates a new logger.\n",
    "            rl_algo_class: Off-policy RL algorithm to use.\n",
    "            rl_kwargs: Keyword arguments to pass to the RL algorithm constructor.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if `dqn_kwargs` includes a key\n",
    "                `replay_buffer_class` or `replay_buffer_kwargs`.\n",
    "        \"\"\"\n",
    "        self.venv = venv\n",
    "\n",
    "        if rl_kwargs is None:\n",
    "            rl_kwargs = {}\n",
    "        # SOMEDAY(adam): we could support users specifying their own replay buffer\n",
    "        # if we made SQILReplayBuffer a more flexible wrapper. Does not seem worth\n",
    "        # the added complexity until we have a concrete use case, however.\n",
    "        if \"replay_buffer_class\" in rl_kwargs:\n",
    "            raise ValueError(\n",
    "                \"SQIL uses a custom replay buffer: \"\n",
    "                \"'replay_buffer_class' not allowed.\",\n",
    "            )\n",
    "        if \"replay_buffer_kwargs\" in rl_kwargs:\n",
    "            raise ValueError(\n",
    "                \"SQIL uses a custom replay buffer: \"\n",
    "                \"'replay_buffer_kwargs' not allowed.\",\n",
    "            )\n",
    "\n",
    "        self.rl_algo = rl_algo_class(\n",
    "            policy=policy,\n",
    "            env=venv,\n",
    "            replay_buffer_class=SQILReplayBuffer,\n",
    "            replay_buffer_kwargs={\"demonstrations\": demonstrations},\n",
    "            **rl_kwargs,\n",
    "        )\n",
    "\n",
    "        super().__init__(demonstrations=demonstrations, custom_logger=custom_logger)\n",
    "\n",
    "    def set_demonstrations(self, demonstrations: algo_base.AnyTransitions) -> None:\n",
    "        assert isinstance(self.rl_algo.replay_buffer, SQILReplayBuffer)\n",
    "        self.rl_algo.replay_buffer.set_demonstrations(demonstrations)\n",
    "\n",
    "    def train(self, *, total_timesteps: int, tb_log_name: str = \"SQIL\", **kwargs: Any):\n",
    "        self.rl_algo.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            tb_log_name=tb_log_name,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def policy(self) -> policies.BasePolicy:\n",
    "        assert isinstance(self.rl_algo.policy, policies.BasePolicy)\n",
    "        return self.rl_algo.policy\n",
    "\n",
    "\n",
    "class SQILReplayBuffer(buffers.ReplayBuffer):\n",
    "    \"\"\"A replay buffer that injects 50% expert demonstrations when sampling.\n",
    "\n",
    "    This buffer is fundamentally the same as ReplayBuffer,\n",
    "    but it includes an expert demonstration internal buffer.\n",
    "    When sampling a batch of data, it will be 50/50 expert and collected data.\n",
    "\n",
    "    It can be used in off-policy algorithms like DQN/SAC/TD3.\n",
    "\n",
    "    Here it is used as part of SQIL, where it is used to train a DQN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        demonstrations: algo_base.AnyTransitions,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        n_envs: int = 1,\n",
    "        optimize_memory_usage: bool = False,\n",
    "    ):\n",
    "        \"\"\"Create a SQILReplayBuffer instance.\n",
    "\n",
    "        Args:\n",
    "            buffer_size: Max number of elements in the buffer\n",
    "            observation_space: Observation space\n",
    "            action_space: Action space\n",
    "            demonstrations: Expert demonstrations.\n",
    "            device: PyTorch device.\n",
    "            n_envs: Number of parallel environments. Defaults to 1.\n",
    "            optimize_memory_usage: Enable a memory efficient variant\n",
    "                of the replay buffer which reduces by almost a factor two\n",
    "                the memory used, at a cost of more complexity.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            buffer_size=buffer_size,\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "            device=device,\n",
    "            n_envs=n_envs,\n",
    "            optimize_memory_usage=optimize_memory_usage,\n",
    "            handle_timeout_termination=False,\n",
    "        )\n",
    "\n",
    "        self.expert_buffer = buffers.ReplayBuffer(\n",
    "            buffer_size=0,\n",
    "            observation_space=observation_space,\n",
    "            action_space=action_space,\n",
    "        )\n",
    "        self.set_demonstrations(demonstrations)\n",
    "\n",
    "    def set_demonstrations(\n",
    "        self,\n",
    "        demonstrations: algo_base.AnyTransitions,\n",
    "    ) -> None:\n",
    "        \"\"\"Set the expert demonstrations to be injected when sampling from the buffer.\n",
    "\n",
    "        Args:\n",
    "            demonstrations (algo_base.AnyTransitions): Expert demonstrations.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If `demonstrations` is not a transitions object\n",
    "                or a list of trajectories.\n",
    "        \"\"\"\n",
    "        # If demonstrations is a list of trajectories,\n",
    "        # flatten it into a list of transitions\n",
    "        if not isinstance(demonstrations, types.Transitions):\n",
    "            (\n",
    "                item,\n",
    "                demonstrations,\n",
    "            ) = util.get_first_iter_element(  # type: ignore[assignment]\n",
    "                demonstrations,  # type: ignore[assignment]\n",
    "            )\n",
    "            if isinstance(item, types.Trajectory):\n",
    "                demonstrations = rollout.flatten_trajectories(\n",
    "                    demonstrations,  # type: ignore[arg-type]\n",
    "                )\n",
    "\n",
    "        if not isinstance(demonstrations, types.Transitions):\n",
    "            raise NotImplementedError(\n",
    "                f\"Unsupported demonstrations type: {demonstrations}\",\n",
    "            )\n",
    "\n",
    "        n_samples = len(demonstrations)\n",
    "        self.expert_buffer = buffers.ReplayBuffer(\n",
    "            buffer_size=n_samples,\n",
    "            observation_space=self.observation_space,\n",
    "            action_space=self.action_space,\n",
    "            handle_timeout_termination=False,\n",
    "        )\n",
    "\n",
    "        for transition in demonstrations:\n",
    "            self.expert_buffer.add(\n",
    "                obs=transition[\"obs\"],\n",
    "                next_obs=transition[\"next_obs\"],\n",
    "                action=transition[\"acts\"],\n",
    "                done=transition[\"dones\"],\n",
    "                reward=np.array(1.0),\n",
    "                infos=[{}],\n",
    "            )\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: np.ndarray,\n",
    "        done: np.ndarray,\n",
    "        infos: List[Dict[str, Any]],\n",
    "    ) -> None:\n",
    "        super().add(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            action=action,\n",
    "            reward=np.array(0.0),\n",
    "            done=done,\n",
    "            infos=infos,\n",
    "        )\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        env: Optional[vec_env.VecNormalize] = None,\n",
    "    ) -> buffers.ReplayBufferSamples:\n",
    "        \"\"\"Sample a batch of data.\n",
    "\n",
    "        Half of the batch will be from expert transitions,\n",
    "        and the other half will be from the learner transitions.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Number of elements to sample in total\n",
    "            env: associated gym VecEnv to normalize the observations/rewards\n",
    "                when sampling\n",
    "\n",
    "        Returns:\n",
    "            A mix of transitions from the expert and from the learner.\n",
    "        \"\"\"\n",
    "        new_sample_size, expert_sample_size = util.split_in_half(batch_size)\n",
    "        new_sample = super().sample(new_sample_size, env)\n",
    "        expert_sample = self.expert_buffer.sample(expert_sample_size, env)\n",
    "\n",
    "        return type_aliases.ReplayBufferSamples(\n",
    "            *(\n",
    "                th.cat((getattr(new_sample, name), getattr(expert_sample, name)))\n",
    "                for name in new_sample._fields\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      8\u001b[0m venv \u001b[38;5;241m=\u001b[39m make_vec_env(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     rng\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed\u001b[38;5;241m=\u001b[39mSEED),\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m sqil_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSQIL\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdemonstrations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpert_trajectories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#policy=\"MlpPolicy\",\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMultiInputPolicy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrl_algo_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrl_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mSQIL.__init__\u001b[0;34m(self, venv, demonstrations, policy, custom_logger, rl_algo_class, rl_kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rl_kwargs:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQIL uses a custom replay buffer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay_buffer_kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_algo \u001b[38;5;241m=\u001b[39m \u001b[43mrl_algo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSQILReplayBuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemonstrations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemonstrations\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrl_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(demonstrations\u001b[38;5;241m=\u001b[39mdemonstrations, custom_logger\u001b[38;5;241m=\u001b[39mcustom_logger)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:157\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[0;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef_optimizer: Optional[th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:160\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:483\u001b[0m, in \u001b[0;36mMultiInputPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    466\u001b[0m     observation_space: spaces\u001b[38;5;241m.\u001b[39mSpace,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m     share_features_extractor: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    482\u001b[0m ):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_std_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_expln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_critics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshare_features_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:278\u001b[0m, in \u001b[0;36mSACPolicy.__init__\u001b[0;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, use_sde, log_std_init, use_expln, clip_mean, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs, n_critics, share_features_extractor)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    269\u001b[0m     {\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_critics\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_critics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     }\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor \u001b[38;5;241m=\u001b[39m share_features_extractor\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:281\u001b[0m, in \u001b[0;36mSACPolicy._build\u001b[0;34m(self, lr_schedule)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr_schedule: Schedule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    284\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/policies.py:342\u001b[0m, in \u001b[0;36mSACPolicy.make_actor\u001b[0;34m(self, features_extractor)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features_extractor: Optional[BaseFeaturesExtractor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Actor:\n\u001b[0;32m--> 342\u001b[0m     actor_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Actor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mactor_kwargs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/policies.py:114\u001b[0m, in \u001b[0;36mBaseModel._update_features_extractor\u001b[0;34m(self, net_kwargs, features_extractor)\u001b[0m\n\u001b[1;32m    111\u001b[0m net_kwargs \u001b[38;5;241m=\u001b[39m net_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# The features extractor is not shared, create a new one\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     features_extractor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m net_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(features_extractor\u001b[38;5;241m=\u001b[39mfeatures_extractor, features_dim\u001b[38;5;241m=\u001b[39mfeatures_extractor\u001b[38;5;241m.\u001b[39mfeatures_dim))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_features_extractor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseFeaturesExtractor:\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/torch_layers.py:259\u001b[0m, in \u001b[0;36mCombinedExtractor.__init__\u001b[0;34m(self, observation_space, cnn_output_dim, normalized_image)\u001b[0m\n\u001b[1;32m    256\u001b[0m extractors: Dict[\u001b[38;5;28mstr\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    258\u001b[0m total_concat_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, subspace \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_image_space(subspace, normalized_image\u001b[38;5;241m=\u001b[39mnormalized_image):\n\u001b[1;32m    261\u001b[0m         extractors[key] \u001b[38;5;241m=\u001b[39m NatureCNN(subspace, features_dim\u001b[38;5;241m=\u001b[39mcnn_output_dim, normalized_image\u001b[38;5;241m=\u001b[39mnormalized_image)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'spaces'"
     ]
    }
   ],
   "source": [
    "\n",
    "from imitation.util.util import make_vec_env\n",
    "import numpy as np\n",
    "from stable_baselines3 import sac, dqn\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "venv = make_vec_env(\n",
    "    \"Pendulum-v1\",\n",
    "    rng=np.random.default_rng(seed=SEED),\n",
    ")\n",
    "\n",
    "sqil_trainer = SQIL(\n",
    "    venv=venv,\n",
    "    demonstrations=expert_trajectories,\n",
    "    #policy=\"MlpPolicy\",\n",
    "    policy='MultiInputPolicy',\n",
    "    rl_algo_class=sac.SAC,\n",
    "    rl_kwargs=dict(seed=SEED),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the untrained policy only gets poor rewards (< 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward_before_training, _ = evaluate_policy(sqil_trainer.policy, venv, 100)\n",
    "print(f\"Reward before training: {reward_before_training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can observe that agent is quite improved (> 1000), although it does not reach the expert performance in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqil_trainer.train(\n",
    "    total_timesteps=1000,\n",
    ")  # Note: set to 300_000 to obtain good results\n",
    "reward_after_training, _ = evaluate_policy(sqil_trainer.policy, venv, 100)\n",
    "print(f\"Reward after training: {reward_after_training}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
